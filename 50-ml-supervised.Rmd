# Supervised Learning 



Reference: https://lgatto.github.io/IntroMachineLearningWithR/supervised-learning.html

See topics:

- Introduction
- Preview
- Model performance
- Cross-validation
- Classification performance
  - Confusion matrix
- Random forest
  - Decision tree
- Data preprocessing
  - Missing values
  - RSME
  - Median imputation

## Test v. Training Data

A key concept we will start using in this section is splitting data into 2 separate tibbles. One will be used to training our data, and the other to test the resulting model. This helps us keep from over-training our data.

We will normally use the function `createDataPartition` from the `caret` library. 

```{r, eval=FALSE, echo=TRUE}
library(caret)
library(tidyverse)

full_tibble <- tibble(
  from = seq(1, 100),
  to = seq(100, 1)
)

# Get a vector of integers, referring to the rows that will be in our training data.
index_of_training_rows <- createDataPartition( 
    full_tibble$from,  # a vector from our data
    p = 0.3,           # percentage of data for the training set
    list = FALSE,      # don't return a list of the 0 or 1 values
    times = 1          # how many splits?
    )

# Make our two new tibbles. One for training, one or testing.

t_train = full_tibble[  index_of_training_rows, ] # Note the comma!
t_test  = full_tibble[ -index_of_training_rows, ] # Note the - and the comma!

```


## Linear Regression

Linear Regression is an incredibly important tool. It uses independent variables to predict a single output/dependent variable.

Outcomes:

- Define the goal of linear regression
- Describe the types of data it works well on
- Define the terms: r^2, p value, residuals

Help:

- [Video on multiple regression in R](https://www.youtube.com/watch?v=hokALdIst8k)
- [Video on multiple regression in theory](https://www.youtube.com/watch?v=zITIFTsivN8)
- [Multiple linear regression tutorial](https://bradleyboehmke.github.io/HOML/linear-regression.html)


### Minimal Example

```{r, eval=FALSE, echo=TRUE}
library(tidyverse)

t_data <- tibble(
  from = c(1, 2, 3, 4, 5, 6, 7, 8, 9, 10),
  to = c(10, 10, 8, 8, 7, 6, 5, 4, 2, 1)
)


# Quick viz of data showing the relationship.
pairs( t_data )

# Create a linear regression model predicting `to` with `from` and using the entire dataset.
model = lm(to ~ from, data = t_data)


# Show a summary of the linear model. Pay attention to these key items:
#   Residuals = errors in each row between actual & predicted
#   Std Error = the squared difference between
#      the predicted and actual values. 
#   Coefficients: 
#     Estimate: value of the change in input to output
#     Std. Error: averages squared diff between prediction / actual
#     p Value: probability of the estimate being a result of random chance
#   Residual standard error: the overall avg difference between actual
#     and predicted for the entire model. 
#   Adj R^2,  the % of variation explained by the model
summary(model)
```

### Full example

```{r, eval=FALSE, echo=TRUE}
library(tidyverse)

t_data <- tibble(
  from = c(1, 2, 3, 4, 5, 6, 7, 8, 9, 10),
  to = c(10, 10, 8, 8, 7, 6, 5, 4, 2, 1)
)

# Quick viz of data showing the relationship. Look for any obvious correlation values.
# We cant to avoid any perfectly correlated data, as that will cause problems in our regression.
pairs( t_data )

# Show above, but in numbers.
cor( t_data, use = 'pairwise.complete.obs' )

# Split our data into test / train
row_indexes_to_include_in_train <- createDataPartition( 
   t_data$from,
   p = .5, # include 1/2 of data in each set.
   list = FALSE, # don't return as a list
   times = 1) # have 1 split

t_train <- t_data[ row_indexes_to_include_in_train, ]
t_test <- t_data[ -row_indexes_to_include_in_train, ]

# Create a linear regression model predicting `to` with `from` and using the train dataset.
model = lm(to ~ from, data = t_train)

# Show a summary of the linear model. Pay attention to these key items:
#   Residuals = errors in each row between actual & predicted
#   Std Error = the squared difference between
#      the predicted and actual values. 
#   Coefficients: 
#     Estimate: value of the change in input to output
#     Std. Error: averages squared diff between prediction / actual
#     p Value: probability of the estimate being a result of random chance
#   Residual standard error: the overall avg difference between actual
#     and predicted for the entire model. 
#   Adj R^2,  the % of variation explained by the model
summary(model)

# Show a plot of residuals / errors
hist(model$residuals)

# If we plug in a 95% confidence interval, how accurate are we?
confint(model, level = 0.95)


# Now look at how the data performs on the test data.
# This function will take in a tibble and a model, test it,
#   and calculate the resulting average error per item.
rmse <- function( m, tibble, dependent_variable ) {
  results <- predict(m, tibble)
  errors <- results - dependent_variable
  return( sqrt(mean(errors^2, na.rm = TRUE)))
}

rmse(model, t_train, t_train$to)
rmse(model, t_test, t_test$to)

```