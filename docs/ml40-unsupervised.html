<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />




<title>Unsupervised learning</title>

<script src="site_libs/header-attrs-2.28/header-attrs.js"></script>
<script src="site_libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/united.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<style>h1 {font-size: 34px;}
       h1.title {font-size: 38px;}
       h2 {font-size: 30px;}
       h3 {font-size: 24px;}
       h4 {font-size: 18px;}
       h5 {font-size: 16px;}
       h6 {font-size: 12px;}
       code {color: inherit; background-color: rgba(0, 0, 0, 0.04);}
       pre:not([class]) { background-color: white }</style>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<link href="site_libs/highlightjs-9.12.0/default.css" rel="stylesheet" />
<script src="site_libs/highlightjs-9.12.0/highlight.js"></script>
<style>
@media print {
  .tab-pane {
    display: block !important;
    opacity: 1;
  }
  /* do not show tabset navigation */
  ul.nav {
    display: none !important;
  }
  /* show headers when printing */
  .printhead {
    display: block !important;
    opacity 1 !important;
  }
}

/* dont show the headers on screens */
@media screen {
  .printhead {
    display: none;
    opacity: 0;
  }
}
</style>

<script>
$(document).ready(function() {
  var $tabsets = $("div.tabset");  // get all tabsets
  $tabsets.each(function() {       // for each tabset...
    var tabNames = $(this).find("ul.nav-pills > li")  // get all the tab-pane titles...
                          .map(function() {
                            return $.trim($(this).text());
                          }).get();
    $(this).find(".tab-content > .tab-pane").each(function(key, value) { // and for each tab-pane, prepend the corresponding title
      $(this).prepend("<h2 class=\"printhead\">" + tabNames[key] + "</h2>");
    });
  });
});
</script>

<style type="text/css">
  code{white-space: pre-wrap;}
  span.smallcaps{font-variant: small-caps;}
  span.underline{text-decoration: underline;}
  div.column{display: inline-block; vertical-align: top; width: 50%;}
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  ul.task-list{list-style: none;}
    </style>

<style type="text/css">code{white-space: pre;}</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>









<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
img {
  max-width:100%;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
details > summary > p:only-child {
  display: inline;
}
pre code {
  padding: 0;
}
</style>


<style type="text/css">
.dropdown-submenu {
  position: relative;
}
.dropdown-submenu>.dropdown-menu {
  top: 0;
  left: 100%;
  margin-top: -6px;
  margin-left: -1px;
  border-radius: 0 6px 6px 6px;
}
.dropdown-submenu:hover>.dropdown-menu {
  display: block;
}
.dropdown-submenu>a:after {
  display: block;
  content: " ";
  float: right;
  width: 0;
  height: 0;
  border-color: transparent;
  border-style: solid;
  border-width: 5px 0 5px 5px;
  border-left-color: #cccccc;
  margin-top: 5px;
  margin-right: -10px;
}
.dropdown-submenu:hover>a:after {
  border-left-color: #adb5bd;
}
.dropdown-submenu.pull-left {
  float: none;
}
.dropdown-submenu.pull-left>.dropdown-menu {
  left: -100%;
  margin-left: 10px;
  border-radius: 6px 0 6px 6px;
}
</style>

<script type="text/javascript">
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark the anchor link active (and if it's in a dropdown, also mark that active)
  var dropdown = menuAnchor.closest('li.dropdown');
  if (window.bootstrap) { // Bootstrap 4+
    menuAnchor.addClass('active');
    dropdown.find('> .dropdown-toggle').addClass('active');
  } else { // Bootstrap 3
    menuAnchor.parent().addClass('active');
    dropdown.addClass('active');
  }

  // Navbar adjustments
  var navHeight = $(".navbar").first().height() + 15;
  var style = document.createElement('style');
  var pt = "padding-top: " + navHeight + "px; ";
  var mt = "margin-top: -" + navHeight + "px; ";
  var css = "";
  // offset scroll position for anchor links (for fixed navbar)
  for (var i = 1; i <= 6; i++) {
    css += ".section h" + i + "{ " + pt + mt + "}\n";
  }
  style.innerHTML = "body {" + pt + "padding-bottom: 40px; }\n" + css;
  document.head.appendChild(style);
});
</script>

<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before, .tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "\e259";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "\e258";
  font-family: 'Glyphicons Halflings';
  border: none;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
  background-color: transparent;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<!-- code folding -->




</head>

<body>


<div class="container-fluid main-container">




<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-bs-toggle="collapse" data-target="#navbar" data-bs-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">EDA Textbook</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="index.html">Home</a>
</li>
<li>
  <a href="r00-index.html">R</a>
</li>
<li>
  <a href="sql00-index.html">SQL</a>
</li>
<li>
  <a href="dv00-index.html">Tableau/PowerBI</a>
</li>
<li>
  <a href="http://ecampus.wvu.edu">eCampus</a>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div id="header">



<h1 class="title toc-ignore">Unsupervised learning</h1>

</div>

<div id="TOC">
<ul>
<li><a href="#introduction" id="toc-introduction">Introduction</a></li>
<li><a href="#k-means" id="toc-k-means">k-means</a>
<ul>
<li><a href="#iris-data" id="toc-iris-data">Iris Data</a></li>
<li><a href="#k-means-algorithm-result"
id="toc-k-means-algorithm-result">k-means algorithm result</a></li>
<li><a href="#minimal-example" id="toc-minimal-example">Minimal
example</a></li>
<li><a href="#example-with-nicer-plot-showing-clusters"
id="toc-example-with-nicer-plot-showing-clusters">Example with nicer
plot showing clusters</a></li>
<li><a href="#example-with-na-handling"
id="toc-example-with-na-handling">Example with NA handling</a></li>
</ul></li>
<li><a href="#decision-trees" id="toc-decision-trees">Decision Trees</a>
<ul>
<li><a href="#concept" id="toc-concept">Concept</a></li>
<li><a href="#minimal-example-1" id="toc-minimal-example-1">Minimal
Example</a></li>
<li><a href="#full-example" id="toc-full-example">Full example</a></li>
</ul></li>
<li><a href="#principal-component-analysis-pca"
id="toc-principal-component-analysis-pca">Principal Component Analysis
(pca)</a>
<ul>
<li><a href="#prior-concept" id="toc-prior-concept">Prior
Concept</a></li>
<li><a href="#concept-walk-through"
id="toc-concept-walk-through">Concept Walk-through</a></li>
<li><a href="#concept-illustration"
id="toc-concept-illustration">Concept Illustration</a></li>
<li><a href="#full-example-1" id="toc-full-example-1">Full
example</a></li>
</ul></li>
</ul>
</div>

<div id="introduction" class="section level1">
<h1>Introduction</h1>
<p>Cluster dataset: - Board Game Geek Data - <a
href="https://github.com/rfordatascience/tidytuesday/tree/master/data/2019/2019-03-12"
class="uri">https://github.com/rfordatascience/tidytuesday/tree/master/data/2019/2019-03-12</a></p>
<p>Error in weather predictions <a
href="https://github.com/rfordatascience/tidytuesday/tree/master/data/2022/2022-12-20"
class="uri">https://github.com/rfordatascience/tidytuesday/tree/master/data/2022/2022-12-20</a></p>
</div>
<div id="k-means" class="section level1 tabset">
<h1 class="tabset">k-means</h1>
<p>k-means clustering is an technique that automatically assigns
individual data points to groups. It’s useful for finding natural groups
in your data.</p>
<p>Here are some resources:</p>
<ul>
<li>Conceptual overview: <a
href="https://k-means-explorable.vercel.app/">k-means: an explorable
explainer</a></li>
<li>R code walkthrough: <a
href="https://lgatto.github.io/IntroMachineLearningWithR/unsupervised-learning.html#k-means-clustering">Unsupervised
Learning: k-means</a></li>
<li>Deep dive <a
href="https://bradleyboehmke.github.io/HOML/kmeans.html">Hands-on
machine learning with R</a></li>
</ul>
<p>We will be using the iris dataset extensively.</p>
<div class="float">
<img
src="https://upload.wikimedia.org/wikipedia/commons/thumb/9/9f/Iris_virginica.jpg/1280px-Iris_virginica.jpg"
alt="Wikipedia Picture of an Iris" />
<div class="figcaption">Wikipedia Picture of an Iris</div>
</div>
<div id="iris-data" class="section level2">
<h2>Iris Data</h2>
<p>We are going to try and use the sepal/petal lengths/widths to
automatically identify the 3 different types of iris flowers.</p>
<p>Below shows one sample plot, which compares the Sepal Length against
the Petal Length. But, we have a lot of possible combinations.</p>
<pre class="r"><code>library(tidyverse)

# Load iris data into a tibble
data(&quot;iris&quot;)
t_iris &lt;- tibble(iris)

# Plot petals
ggplot( data=t_iris, mapping = aes( x=Sepal.Length, y=Petal.Length, color=Species)) +
  geom_point() + 
  labs( title = &#39;Iris dataset: species variable&#39;)</code></pre>
<p><img src="ml40-unsupervised_files/figure-html/unnamed-chunk-2-1.png" width="672" /></p>
</div>
<div id="k-means-algorithm-result" class="section level2">
<h2>k-means algorithm result</h2>
<p>We can use k-means to see which combination of attributes lets us
have the highest accuracy.</p>
<pre class="r"><code>library(tidyverse)

# Load iris data into a tibble
data(&quot;iris&quot;)
t_iris &lt;- tibble(iris)

# Run the kmeans algorithm:
#   x is a tibble, which should only have numeric data
#   centers is the number of clusters we want
#   nstart says how many random starting points we should try out (default to 10)
kresult &lt;- kmeans(
  x = select(t_iris, Sepal.Length, Petal.Length),
  centers = 3,
  nstart = 10
)

print(kresult)</code></pre>
<pre><code>## K-means clustering with 3 clusters of sizes 41, 58, 51
## 
## Cluster means:
##   Sepal.Length Petal.Length
## 1     6.839024     5.678049
## 2     5.874138     4.393103
## 3     5.007843     1.492157
## 
## Clustering vector:
##   [1] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3
##  [38] 3 3 3 3 3 3 3 3 3 3 3 3 3 1 2 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2
##  [75] 2 2 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 2 1 2 1 1 1 1 2 1 1 1 1
## [112] 1 1 2 2 1 1 1 1 2 1 2 1 2 1 1 2 2 1 1 1 1 1 1 1 1 1 1 2 1 1 1 2 1 1 1 2 1
## [149] 1 2
## 
## Within cluster sum of squares by cluster:
## [1] 20.407805 23.508448  9.893725
##  (between_SS / total_SS =  90.5 %)
## 
## Available components:
## 
## [1] &quot;cluster&quot;      &quot;centers&quot;      &quot;totss&quot;        &quot;withinss&quot;     &quot;tot.withinss&quot;
## [6] &quot;betweenss&quot;    &quot;size&quot;         &quot;iter&quot;         &quot;ifault&quot;</code></pre>
<p>Look at the results of k-means to see how well the algorithm
worked.</p>
<p>There are several useful outputs:</p>
<ul>
<li><code>cluster</code>: vector of numbers, one per input row, showing
the cluster for each row. Use this to assign each of your original data
points to a cluster.</li>
<li><code>centers</code>: matrix of cluster centers. Use this to plot
the center of each cluster on a chart.</li>
<li><code>tot.withiness</code>: total within cluster sum of squares,
i.e. <code>sum(withinss)</code>. We want this to be as small, so each
cluster is tightly defined.</li>
<li><code>between</code>: between-cluster sum of squares. We want this
to be large, showing that each cluster is far away from the other
clusters.</li>
<li><code>size</code>: number of points in each cluster</li>
<li><code>iter</code>: number of times the algorithm ran before
stopping</li>
</ul>
<p>To see a visualization of sum of square, see <a
href="https://shiny.rit.albany.edu/stat/visualizess/"
class="uri">https://shiny.rit.albany.edu/stat/visualizess/</a></p>
</div>
<div id="minimal-example" class="section level2">
<h2>Minimal example</h2>
<p>This is a minimal example, but can be significantly improved in the
full template.</p>
<pre class="r"><code>library(tidyverse)

# Load iris data into a tibble
data(&quot;iris&quot;)
t_iris &lt;- tibble(iris)

# Run the kmeans algorithm:
kresult &lt;- kmeans(
  x = select(t_iris, Sepal.Length, Petal.Length),
  centers = 3,
)

# Add the new kmeans2 column, using kresult&#39;s cluster vector.
t_iris &lt;- mutate(t_iris, kmeans = (kresult$cluster))

# Plot petals
ggplot(data = t_iris) + 
  geom_point(mapping = aes( x = Sepal.Length, y = Petal.Length, color = kmeans) 
  )</code></pre>
<p><img src="ml40-unsupervised_files/figure-html/unnamed-chunk-4-1.png" width="672" /></p>
</div>
<div id="example-with-nicer-plot-showing-clusters"
class="section level2">
<h2>Example with nicer plot showing clusters</h2>
<pre class="r"><code>library(tidyverse)

# Load iris data into a tibble
data(&quot;iris&quot;)
t_iris &lt;- tibble(iris)

# Give a seed value so that we get the same results each time
set.seed(1)

# Run the kmeans algorithm:
#   x is a tibble, which should only have numeric data
#   centers is the number of clusters we want
#   nstart says how many random starting points we should try out (default to 10)
kresult &lt;- kmeans(
  x = select(t_iris, Sepal.Length, Petal.Length),
  centers = 3,
  nstart = 10
)

# Add the new kmeans2 column, using kresult&#39;s cluster vector.
# Use factor to turn the cluster from a number to a categorical variable.
t_iris &lt;- mutate(t_iris, kmeans = factor(kresult$cluster))

# Plot petals
ggplot() + 
  # Plot each point
  geom_point(
    data = t_iris, 
    mapping = aes( x = Sepal.Length, y = Petal.Length, color = kmeans) 
  ) +
  # Plot the center of each cluster 
  geom_point( 
    data = as_tibble(kresult$centers), 
    mapping = aes( x = Sepal.Length, y = Petal.Length), 
    size = 6, 
    shape = 3) +
  labs( 
    title = &#39;Iris dataset: k-means clustering&#39;,
    subtitle = paste(&#39;Error:&#39;, round(kresult$tot.withinss,2))
    )</code></pre>
<p><img src="ml40-unsupervised_files/figure-html/unnamed-chunk-5-1.png" width="672" /></p>
</div>
<div id="example-with-na-handling" class="section level2">
<h2>Example with NA handling</h2>
<p>We can not use columns with NA values in the kmeans algorithm. We can
either remove the rows with NA values, or impute them.</p>
<pre class="r"><code>library(tidyverse)
library(caret)

# Load iris data into a tibble
data(&quot;iris&quot;)

# Make a tibble with some NA values
t_imputed &lt;- select(tibble(iris), -Species)
t_imputed[2,3] &lt;- NA
t_imputed[2,4] &lt;- NA


# Method 1: Interpolate means for na values in tibble for a single column
t_imputed &lt;- t_imputed %&gt;% 
  mutate(Sepal.Length = ifelse(is.na(Sepal.Length), 
                                mean(Sepal.Length, na.rm = TRUE), 
                                Sepal.Length))

# Method 2: Use Caret library
preProc &lt;- preProcess(t_imputed, method = &quot;medianImpute&quot;)
t_imputed &lt;- predict(preProc, t_imputed)

# Run the kmeans algorithm:
kresult &lt;- kmeans(
  x = t_imputed,
  centers = 4,
  nstart = 10
)

# Add the new kmeans2 column, using kresult&#39;s cluster vector.
# Use factor to turn the cluster from a number to a categorical variable.
t_imputed &lt;- mutate(t_imputed, kmeans = factor(kresult$cluster))


# Add the new kmeans2 column, using kresult&#39;s cluster vector.
ggplot() + 
  # Plot each point
  geom_point(
    data = t_imputed, 
    mapping = aes( x = Sepal.Length, y = Petal.Length, color = kmeans) 
  ) +
  # Plot the center of each cluster 
  geom_point( 
    data = as_tibble(kresult$centers), 
    mapping = aes( x = Sepal.Length, y = Petal.Length), 
    size = 6, 
    shape = 3) +
  labs( 
    title = &#39;Iris dataset: k-means clustering&#39;,
    subtitle = paste(&#39;Error:&#39;, round(kresult$tot.withinss,2))
    )</code></pre>
<p><img src="ml40-unsupervised_files/figure-html/unnamed-chunk-6-1.png" width="672" /></p>
</div>
</div>
<div id="decision-trees" class="section level1 tabset">
<h1 class="tabset">Decision Trees</h1>
<p>Decision-trees automatically create questions to split our data into
groups. They are very useful for creating human-understandable models of
a problem space.</p>
<p><em><a
href="http://www.r2d3.us/visual-intro-to-machine-learning-part-1/">The
r2d3 website</a> is a great resources for visualizing how a tree is
built.</em></p>
<p>Other helpful resources:</p>
<ul>
<li><a
href="https://explained.ai/decision-tree-viz/index.html">Visualizations
of DT</a></li>
<li><a
href="https://lgatto.github.io/IntroMachineLearningWithR/supervised-learning.html">Supervised
Learning</a></li>
<li><a
href="https://bradleyboehmke.github.io/HOML/DT.html">HOML</a></li>
</ul>
<div id="concept" class="section level2">
<h2>Concept</h2>
<p>We follow this process:</p>
<ul>
<li>Select an output variable</li>
<li>Go through each input variable to find the best split in our
output</li>
<li>Split the dataset</li>
<li>For each split, repeat until we reach a good ending condition</li>
</ul>
</div>
<div id="minimal-example-1" class="section level2">
<h2>Minimal Example</h2>
<p>Below creates a visualization of a decision tree.</p>
<p>Note the output for each node gives the 0.3 (proportion) of items in
each class, as well as the % of rows that find their way to this
node.</p>
<pre class="r"><code>library(tidyverse)
library(rpart)
library(rpart.plot)

# Load data into a tibble
data(iris)
t.iris &lt;- iris

# Create the model
#   formula: output_variable ~ input_field_a + input_field_b + ...
#   data: your tibble, excluding the output variable.
#   method: what type of problem are we working with?
#     class - predicting a discrete variable
#     anova - regression for a value
m &lt;- rpart(formula = Species ~ Sepal.Length + 
                      Sepal.Width + Petal.Length + Petal.Width, 
           data = t.iris,
           method = &quot;class&quot;)

# Plot results of model
rpart.plot(m)</code></pre>
<p><img src="ml40-unsupervised_files/figure-html/unnamed-chunk-7-1.png" width="672" /></p>
<p>What if we want the output of the plot as a number showing its
accuracy? Add the following lines</p>
<pre class="r"><code># Give the tibble to the model, and generate a vector of predicted
# output class
predicted &lt;- predict(m, t.iris, type = &#39;class&#39;)

# Add the predicted results to each row in the tibble as a field.
predicted_as_str &lt;- paste(&#39;Predicted&#39;, predicted)
t.iris &lt;- mutate( t.iris, predicted = predicted_as_str )

# Show a confusion matrix
table(t.iris$predicted, t.iris$Species)</code></pre>
<pre><code>##                       
##                        setosa versicolor virginica
##   Predicted setosa         50          0         0
##   Predicted versicolor      0         49         5
##   Predicted virginica       0          1        45</code></pre>
</div>
<div id="full-example" class="section level2">
<h2>Full example</h2>
<p>Several other features are very helpful in pruning our decision
tree.</p>
<pre class="r"><code>library(tidyverse, rpart, rpart.plot)

# Load data into a tibble
data(iris)
t.iris &lt;- iris


## Split the data into training / testing

# Find the number of rows
count_of_rows &lt;- length(t.iris$Species)
# Create a random vector of 1s and 0s the same length
select01 &lt;- rbinom(count_of_rows, 1, .5)


## Create a training / test set

# Add the 01 vector to the original tibble
t.iris &lt;- mutate(t.iris, test01 = select01) 
# Create a test and train tibble with this as a filter
t.iris.test &lt;- filter(t.iris, test01 == 1)
t.iris.train &lt;- filter(t.iris, test01 == 0)


## Create the model with TRAIN tibble
# 
#   minsplit: a number with the min number of rows required for a split
#   minbucket: a number wiht the min number of rows required for a bucket
m &lt;- rpart(formula = Species ~ Sepal.Length + 
                      Sepal.Width + Petal.Length + Petal.Width, 
           data = t.iris.train,
           method = &quot;class&quot;,
           minsplit = 2,
           minbucket = 2)

# Plot results of model with the TRAINING data
rpart.plot(m)</code></pre>
<p><img src="ml40-unsupervised_files/figure-html/unnamed-chunk-9-1.png" width="672" /></p>
<pre class="r"><code>## Predict on the TEST tibble

# Create a vector of prediction results
predicted &lt;- predict(m, t.iris.test, type = &#39;class&#39;)
# Add to our train tibble.
t.iris.test &lt;- mutate(t.iris.test, predicted = predicted)

# Show results in a confusion matrix / table.
# This shows a comparison between the the actual values, and what our algorithm
# predicted (shown in upper case for clarity).
table(str_to_upper(predicted), t.iris.test$Species)</code></pre>
<pre><code>##             
##              setosa versicolor virginica
##   SETOSA         21          0         0
##   VERSICOLOR      0         21         2
##   VIRGINICA       0          1        24</code></pre>
</div>
</div>
<div id="principal-component-analysis-pca"
class="section level1 tabset">
<h1 class="tabset">Principal Component Analysis (pca)</h1>
<p>We use principal component analysis to reduce the number of
dimensions in our data. In simpler terms, we can reduce the number of
values in our data.</p>
<p>See this <a
href="https://www.youtube.com/watch?v=FD4DeN81ODY">visualization</a>
before reading the below.</p>
<p>Some good resources are:</p>
<ul>
<li><a
href="https://lgatto.github.io/IntroMachineLearningWithR/unsupervised-learning.html#introduction">Walkthrough
of Unsupervised Learning</a></li>
<li><a href="https://bradleyboehmke.github.io/HOML/pca.html">More
detailed analysis of PCA</a></li>
</ul>
<div id="prior-concept" class="section level2">
<h2>Prior Concept</h2>
<p>Before getting into PCA, make sure you’re familiar with the concept
of correlations. The code below walks you through an example. The
<code>pairs</code> command will show a plot of various fields against
each other.</p>
<pre class="r"><code>library(ggcorrplot)
data(iris)

# Create a set of pairs of scatterplots showing the relationship
# between all variables.
#   argument1 is a tibble with all numeric columns to show in the plot.
#   col is the color of each species
pairs(
  x = select(tibble(iris), -Species),
  col = iris$Species)</code></pre>
<p><img src="ml40-unsupervised_files/figure-html/unnamed-chunk-10-1.png" width="672" /></p>
<pre class="r"><code># View the correlations between each variable.
# Again, we use the iris dataset without the species column (text), giving
#   the chart only the numeric columns
# Positive correlations shown in blue, negative in red.
# Items not meeting statistical significance are hidden.
ggcorrplot(cor(select( iris, -Species)))</code></pre>
<p><img src="ml40-unsupervised_files/figure-html/unnamed-chunk-10-2.png" width="672" /></p>
</div>
<div id="concept-walk-through" class="section level2">
<h2>Concept Walk-through</h2>
<p>We will run pca on the iris dataset.</p>
<pre class="r"><code># Give prcomp numeric values in the iris tibble.
iris_pca_results &lt;- prcomp(select(tibble(iris), -Species))

# Show new components
summary(iris_pca_results)</code></pre>
<pre><code>## Importance of components:
##                           PC1     PC2    PC3     PC4
## Standard deviation     2.0563 0.49262 0.2797 0.15439
## Proportion of Variance 0.9246 0.05307 0.0171 0.00521
## Cumulative Proportion  0.9246 0.97769 0.9948 1.00000</code></pre>
<pre class="r"><code># Show how they are generated from our original data
print(iris_pca_results)</code></pre>
<pre><code>## Standard deviations (1, .., p=4):
## [1] 2.0562689 0.4926162 0.2796596 0.1543862
## 
## Rotation (n x k) = (4 x 4):
##                      PC1         PC2         PC3        PC4
## Sepal.Length  0.36138659 -0.65658877  0.58202985  0.3154872
## Sepal.Width  -0.08452251 -0.73016143 -0.59791083 -0.3197231
## Petal.Length  0.85667061  0.17337266 -0.07623608 -0.4798390
## Petal.Width   0.35828920  0.07548102 -0.54583143  0.7536574</code></pre>
<p>The summary shows each of our new principal components. Each PC is a
new variable, created by testing various formulas weighting the original
variables.</p>
<p>Each new PC (primary component) is made by multiplying existing
values by the new coefficients. For example, <code>PC1</code> is</p>
<p><span class="math inline">\(.36 * Sepal.Length + -.08 * Sepal.Width +
.85 * Petal.Length + .35 * Petal.Width\)</span></p>
<p>Each new PC variable flattens the data across one dimension. The
exact weights for each variable are chosen to maximize the amount of
variation in the original dataset that is maintained in the new
variable.</p>
<p>The second PC then does the same process again. However, now that the
first has already taken up a bunch of the variation in the dataset, the
new only tries to capture the remaining variation in the data. Think of
the first PC as removing a bunch of the variation, and the following PCs
each try to grab more of the variation until none remains.</p>
<p>For interpreting the data, we want each PC to have the highest
proportion of variance (meaning it capture most of our data). Then, look
at the weights for that PC. What variables drive a PC? That helps you
name it.</p>
</div>
<div id="concept-illustration" class="section level2">
<h2>Concept Illustration</h2>
<p>Now that we have a set of PC, let’s visualize the new dimension on
our data. The chart below visualizes our first primary component.</p>
<pre class="r"><code># Run PCA analysis
iris_pca_results &lt;- prcomp(select(tibble(iris), -Species))

# The results have a new list called X, which gives the PC for each of our
# original dataset. Let&#39;s create a new tibble with the pcs, and then plot it.
plot_data &lt;- tibble(
  pc1 = iris_pca_results$x[,1],
  pc2 = iris_pca_results$x[,2],
  pc3 = iris_pca_results$x[,3],
  Species = iris$Species
)

# Plot PC1
ggplot(data = plot_data) +
  geom_point(mapping = aes( x = pc1, y = 1, color = Species),
             alpha = .2,
             size = 2) +
  labs(title = &quot;PC1 versus species&quot;)</code></pre>
<p><img src="ml40-unsupervised_files/figure-html/unnamed-chunk-12-1.png" width="672" /></p>
<p>What about plotting 2 components?</p>
<pre class="r"><code>ggplot(data = plot_data) +
  geom_point(mapping = aes( x = pc1, y = pc2, color = Species),
             alpha = .3,
             size = 2) +
  labs(title = &quot;PC1 and PC2 versus species&quot;)</code></pre>
<p><img src="ml40-unsupervised_files/figure-html/unnamed-chunk-13-1.png" width="672" /></p>
<p>We can also more closely examine how the new PCs are formed. Examine
the rotation variable. It gives us the degree to which each variable
drives the PCs</p>
<pre class="r"><code># Turn into a usable tibble and add the items name
pca_tibble &lt;- as_tibble(iris_pca_results$rotation) %&gt;% 
  mutate(items = rownames(iris_pca_results$rotation) )

# Turn into long format
pca_tibble_long &lt;- pivot_longer(pca_tibble, 
                                cols = starts_with(&#39;PC&#39;), 
                                names_to = &#39;PCItem&#39;, 
                                values_to = &#39;PCValue&#39;)

# Remove values near zero
pca_tibble_long &lt;- filter( pca_tibble_long, PCValue &gt; .1 | PCValue &lt; -.1 )

# View PCs as a basic dot plot.
ggplot( data = pca_tibble_long ) +
  geom_point( mapping = aes( y = items, x = PCValue, color = PCValue), size = 5) +
  facet_wrap( ~ PCItem, nrow = 1)  +
  scale_color_gradient(low = &#39;red&#39;, high = &#39;green&#39;)</code></pre>
<p><img src="ml40-unsupervised_files/figure-html/unnamed-chunk-14-1.png" width="672" /></p>
</div>
<div id="full-example-1" class="section level2">
<h2>Full example</h2>
<p>In the full example, we generally want to scale the data. This is
needed, as if we have radically different numerical ranges, the biggest
numbers will have more influence on the model. Scaling all of our inputs
according to mean/sd will give them equal impact on the outcome.</p>
<pre class="r"><code>library(tidyverse)

# Load data into a tibble
data(iris)
t_iris &lt;- iris
t_iris_numbers &lt;- select(iris, -Species)

# Give prcomp numeric values in the iris tibble.
# scale: adjust the data so that sd = 1?
# center: should we center data around 0?
iris_pca_results &lt;- prcomp(t_iris_numbers,
                           scale = TRUE,
                           center = TRUE)

# Plot our results
biplot(iris_pca_results)</code></pre>
<p><img src="ml40-unsupervised_files/figure-html/unnamed-chunk-15-1.png" width="672" /></p>
<pre class="r"><code># Show new components
summary(iris_pca_results)</code></pre>
<pre><code>## Importance of components:
##                           PC1    PC2     PC3     PC4
## Standard deviation     1.7084 0.9560 0.38309 0.14393
## Proportion of Variance 0.7296 0.2285 0.03669 0.00518
## Cumulative Proportion  0.7296 0.9581 0.99482 1.00000</code></pre>
<pre class="r"><code># Show how they are generated from our original data
print(iris_pca_results)</code></pre>
<pre><code>## Standard deviations (1, .., p=4):
## [1] 1.7083611 0.9560494 0.3830886 0.1439265
## 
## Rotation (n x k) = (4 x 4):
##                     PC1         PC2        PC3        PC4
## Sepal.Length  0.5210659 -0.37741762  0.7195664  0.2612863
## Sepal.Width  -0.2693474 -0.92329566 -0.2443818 -0.1235096
## Petal.Length  0.5804131 -0.02449161 -0.1421264 -0.8014492
## Petal.Width   0.5648565 -0.06694199 -0.6342727  0.5235971</code></pre>
</div>
</div>




</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.odd').parent('tbody').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- tabsets -->

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open');
  });
});
</script>

<!-- code folding -->


<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
