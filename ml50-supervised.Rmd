---
title: Supervised Learning
output: 
  html_document:
    toc: true
    theme: united
---

# More information

Here is great tutorial for approaches in machine learning: 
https://lgatto.github.io/IntroMachineLearningWithR/supervised-learning.html

This covers these topics:

- Introduction
- Preview
- Model performance
- Cross-validation
- Classification performance
  - Confusion matrix
- Random forest
  - Decision tree
- Data preprocessing
  - Missing values
  - RSME
  - Median imputation

# Decision Trees {.tabset}

Decision trees are a method for predicting an outcome from a set of variables. They are great for creating easily-understandable trees.

## Class Example

This creates a decision tree to predict a textual (class) output.

```{r echo=TRUE, message=FALSE, warning=FALSE}
library(tidyverse)
library(rpart)
library(rpart.plot)

# Load data into a tibble
t <- tibble(
  temp = c(67, 68, 70, 70, 76, 80, 85, 84, 89, 90, 91, 95, 100),
  raining = c('n', 'n', 'n', 'n', 'n','n','n','n','n','n','y', 'n', 'n'),
  camped = c('Camped', 'No', 'No', 'No', 'No', 'No', 'Camped', 'Camped', 'Camped', 'Camped', 'No', 'No', 'No')
)

# Create the model
# https://stat.ethz.ch/R-manual/R-devel/library/rpart/html/rpart.control.html
#   formula: set output to the Species, and choose input fields
#   data: set to our tibble
#   minsplit: the minimum number of observations that must exist in a node in order for a split to be attempted. 
#   minbucket: the minimum number of observations in any terminal
#   method: class, meaning we are predicting a discrete variable
#   
m <- rpart(formula = camped ~ temp + raining,
           data = t,
           minsplit = 8,
           minbucket = 5,
           method = "class")

# Show results of model
rpart.plot(m)

# Create the predicted value and add it to our tibble
predicted <- predict(m, t, type = 'class')
t <- mutate( t, 
             predicted = predicted,
             is_correct = predicted == camped)

# Percentage correct
print(mean(t$is_correct))

# Show a confusion matrix
# Predicted values are in upper case.
table(str_to_upper(t$predicted), t$camped)
```


## Regression Example

This creates a decision tree to predict a numeric output with regression.

```{r echo=TRUE, message=FALSE, warning=FALSE}
library(tidyverse)
library(rpart)
library(rpart.plot)

# Load data into a tibble
t <- tibble(
  temp = c(67, 68, 70, 70, 76, 80, 85, 84, 89, 90, 91, 95, 100),
  raining = c('n', 'n', 'n', 'n', 'n','n','n','n','n','n','y', 'n', 'n'),
  camped = c('Camped', 'No', 'No', 'No', 'No', 'No', 'Camped', 'Camped', 'Camped', 'Camped', 'No', 'No', 'No')
)

# Create the model
# https://stat.ethz.ch/R-manual/R-devel/library/rpart/html/rpart.control.html
#   formula: set output to the Species, and choose input fields
#   data: set to our tibble
#   minsplit: the minimum number of observations that must exist in a node in order for a split to be attempted. 
#   minbucket: the minimum number of observations in any terminal
#   method: class, meaning we are predicting a discrete variable
m <- rpart(formula = temp ~ camped + raining,
           data = t,
           minsplit = 1,
           minbucket = 1,
           method = 'anova')

# Show results of model
rpart.plot(m)

# Create the predicted value and add it to our tibble
predicted <- predict(m, t)
t <- mutate( t, 
             predicted = predicted,
             residual = predicted - temp)

# Residual value
hist(t$residual)
```

# Neural Network - simple class  {.tabset}

Good reference videos: 

- [Neural Network in 5 minutes](https://www.youtube.com/watch?v=bfmFfD2RIcg)
- [Stat Quest: Math in the ANN](https://www.youtube.com/watch?v=CqOfi41LfDw)

We will use the [NeuralNet library](https://www.rdocumentation.org/packages/neuralnet/).

## Build a model

Below is a simple model created from some sample fraud data. It predicts a simple 0/1 outcome.

```{r echo=TRUE, message=FALSE, warning=FALSE}
library(neuralnet)
library(tidyverse)

t_train <- tribble( 
  ~fraud, ~jail01, ~age01, ~grump01,
  1, 1, .5, .5,
  1, 1, 1, 0, 
  1, 1, 0, 1,
  1, 0, .75, .8,
  1, 0, .6, 1,
  1, 1, .5, .9,
  0, 0, 0, 1, 
  0, 0, 0, 0,
  0, 0, 1, 1, 
  0, 0, 1, .8,
  )

t_test <- tribble( 
  ~fraud, ~jail01, ~age01, ~grump01,
  1, 1, 0, 0,
  1, 1, .5, .25, 
  0, 0, 0, 1, 
  0, 0, 0, 0,
  0, 0, .9, .9, 
  0, 0, 7, .5,
)


# Set the random number generator's starting value. This makes our outputs repeatable, so we 
# don't get different results each time we run our software.
set.seed(1)

# The neuralnet function creates our model.
# Arguments:
#   formula: dependent variable ~ indepedent variables + ...
#   data: our tibble
#   hidden: how many hidden nodes should we use? Use an integer
#   linear.output: generally T for regression or F for classification
n <- neuralnet(fraud ~ jail01 + age01,
               data = t_train,
               hidden = 1,
               linear.output = FALSE)

# View output
# Input notes are on the left, intermediate in the center, and output on the right.
# Steps tells us how many times the algorithm iterated through before stopping
# Error is the sse, but other options are also available.
plot(n)
```


## Measure accuracy on training data

```{r echo=TRUE, message=FALSE, warning=FALSE}
# Continue the previous code block, ...


# Measure accuracy on training set
# This code makes a new tibble showing the results in a easy to grasp form.
#   prediction: this is the output of our model. Use net.result[[1]] to grab the first item returned.
#   prediction01: the prediction outputs a decimal number. Since this problem has a 0/1, round.
prediction = n$net.result[[1]]
prediction01 = round(prediction, digits = 0)

# Create a confusion table
# Prediction is on the left, and actual on the top.
#
#             0                1       <===  Actual outcome
# 0    Negative was True    Negative was False
# 1    Positive was False   Positive was True
# ^
#  \  
#    prediction
# 
train_table <- table(prediction01, t_train$fraud )
print(train_table)
```

```{r echo=TRUE, message=FALSE, warning=FALSE}
# Use coordinates to access our truth table
# We start at the top-left corner with [1,1], with [line, column]
# 
#       0        1
# 0   [1, 1]   [1, 2]
# 1   [2, 1]   [2, 2]

# Accuracy: true / total
# What is the total number of cases properly classified?
train_accuracy <- (train_table[1, 1] + train_table[2, 2]) /  sum(train_table)

# Recall: true positives / (true positives + false negative)
# How many of the target cases did we find?
train_recall <- train_table[2, 2] / (train_table[1, 2] + train_table[2, 2])

# Precision: true positives / (false positive + true positive)
train_precision <- train_table[2, 2] / (train_table[2, 1] + train_table[2, 2])

print(paste('Accuracy', train_accuracy))
print(paste('Recall', train_recall))
print(paste('Precision', train_precision))
```

## Evaluate on test data

```{r echo=TRUE, message=FALSE, warning=FALSE}
# Continue the previous code block, ...

# Now try test data.
#   n is our model
#   t_test is our data
p <- predict(n, t_test)

# Evaluate the success by looking at the returned vector
# Note that we get a matrix back. We just want a vector, so access
# by using [ , 1] <--- return all rows, but only first column.
prediction = p[,1]
prediction01 = round(prediction, digits = 0)

confusion <- table(prediction01, t_test$fraud)

print(confusion)
```


# Neural Network - multiple classes {.tabset}

## Build a model

We can use textual data. It will be encoded with each value set to a 0 or 1 field.

```{r echo=TRUE, message=FALSE, warning=FALSE}
library(neuralnet)
library(tidyverse)

# Load our iris dataset
data(iris)

set.seed(1)

# Create a sample vector of 1 and 0s, with 70% 1st and 30% 0. 
index <- sample( c(1, 0), 
                 nrow(iris), 
                 replace = TRUE, 
                 prob = c(.7, .3) )

# Split data into test/train
t_train <- filter( iris, index == 1 ) 
t_test <- filter( iris, index == 0)

# Train algorithm
# Note that we are using a textual output, so instead of a single output node
# we now have one per label.
#   linear.output: generally T for regression or F for classification
n <- neuralnet(Species  ~ Sepal.Length + Sepal.Width + Petal.Length + Petal.Width,
               data = t_train,
               hidden = 2,
               linear.output = FALSE)

plot(n)
```

## Measure accuracy on training data

Below is some sample code to measure the accuracy of the training / test data.

```{r echo=TRUE, message=FALSE, warning=FALSE}
# continue previous...

# Grab the matrix of results and round to the 0
matrix = round(n$net.result[[1]], digits = 0)

# Add column names
colnames(matrix) <- list('setosa_node', 'versicolor_node', 'virginica_node')

# Convert matrix to a tibble
#     Add a column called predicted that says if we are 
#     predicting setota, versicolor, or virginica.
t_results <- as.tibble(matrix)  %>%
  mutate(predicted = ifelse(setosa_node == 1, 'setosa', 
                            ifelse(versicolor_node == 1, 'versicolor', 'virginica'))) 


# Test accuracy with original data
accuracy = mean(t_results$predicted == t_train$Species)
print(accuracy)

# See confusion table
table(str_to_upper(t_results$predicted), t_train$Species)
```
  
## Measure accuracy on test data

Below is some sample code to measure the accuracy of the training / test data.

```{r echo=TRUE, message=FALSE, warning=FALSE}
# continue previous...

predictions <- predict(n, t_test)

# Grab the matrix of results and round to the 0
matrix = round(predictions, digits = 0)

# Add column names
colnames(matrix) <- list('setosa_node', 'versicolor_node', 'virginica_node')

# Convert to tibble
t_results <- as.tibble(matrix)  %>%
  mutate(predicted = ifelse(setosa_node == 1, 'setosa', 
                            ifelse(versicolor_node == 1, 'versicolor', 'virginica'))) 


# Test accuracy with original data
accuracy = mean(t_results$predicted == t_test$Species)
print(accuracy)

# See confusion table
table(t_results$predicted, t_test$Species)
```

# Neural Network - numeric output {.tabset}

## Build a model

Below is a simple model predicting a numerical outcome. We use the mtcars dataset.

```{r echo=TRUE, message=FALSE, warning=FALSE}
library(neuralnet)
library(tidyverse)

t <- tribble( 
  ~height, ~weight, ~is_sumo,
  100, 100, 1, 
  200, 190, 0,
  150, 130, 1, 
  80, 200, 0, 
  175, 200, 1, 
  100, 80, 1,
  90, 70, 1,
  210, 200, 1,
  150, 160, 0,
  110, 140, 0,
  175, 200, 1, 
  103, 80, 1
)

# Scale the data and add a 0/1 
is_test = sample(x = c(0, 1),
                 size = 12,
                 replace = TRUE,
                 prob = c(0.5, 0.5))

# Setup the data to be scaled and have a 1/0 test/train variable
t <- t %>% 
  mutate(is_test = is_test, 
         height = scale(height),
         weight = scale(weight))


# Setup train/test tibbles
t_train <- filter(t, is_test == 0)
t_test <- filter(t, is_test == 1)


# Set the random number generator's starting value. This makes our outputs repeatable, so we 
# don't get different results each time we run our software.
set.seed(1)

# The neuralnet function creates our model.
# Arguments:
#   formula: dependent variable ~ indepedent variables + ...
#   data: our tibble
#   hidden: how many hidden nodes should we use? Use an integer
#   linear.output: generally T for regression or F for classification
n <- neuralnet(formula = height ~ weight + is_sumo,
               data = t_train,
               hidden = 1,
               linear.output = TRUE)

# View output
# Input notes are on the left, intermediate in the center, and output on the right.
# Steps tells us how many times the algorithm iterated through before stopping
# Error is the sse, but other options are also available.
plot(n)
```


## Measure accuracy on training data

```{r echo=TRUE, message=FALSE, warning=FALSE}
# Continue the previous code block, ...


# Measure accuracy on training set
# This code makes a new tibble showing the results in a easy to grasp form.
#   prediction: this is the output of our model. Use net.result[[1]] to grab the first item returned.
prediction = n$net.result[[1]]

# Show error in a nice format
t_train <- t_train %>% 
  mutate(prediction = prediction,
         residual = prediction - weight)

# Show predicted v. actual
print(t_train)

# Find the sse
print((sum(t_train$residual ^ 2)))
```

## Evaluate on test data

```{r echo=TRUE, message=FALSE, warning=FALSE}
# Continue the previous code block, ...

# Now try test data.
#   n is our model
#   t_test is our data
p <- predict(n, t_test)

# Evaluate the success by looking at the returned vector
# Note that we get a matrix back. We just want a vector, so access
# by using [ , 1] <--- return all rows, but only first column.
prediction = p[,1]

# Show error in a nice format
t_test <- t_test %>% 
  mutate(prediction = prediction,
         residual = prediction - weight)

# Show predicted v. actual
print(t_test)

# Find the the sse
print((sum(t_test$residual ^ 2)))


```


# Scaling data

Scale is a useful function for rescaling a set of input data.

``` {r}

# Rescale data
#   x: vector of data input
#   center: boolean, do we subtract column mean from each value (so mean = 0)?
#   scale: boolean, do we divide data by the sd after centering?
x2 <- scale(x = c(1, 2, 3, 4, 4, 5,3),
      center = TRUE,
      scale = TRUE)

summary(x2)

# We can also use the short form of this, relying on the defaults to get the 
# same result as above.
summary(scale(c(1, 2, 3, 4, 4, 5,3)))


# We will often need to rescale the output to get it back into the normal form.
#   attributes:
#     dim
#     scaled:center
#     scaled:scale
attributes(x2)

# Get only the new data
scaled_data <- x2[,1]
print(scaled_data)

# Rescale by multiplying by the scale and adding the center.
scaled_data * attr(x2, 'scaled:scale') + attr(x2, 'scaled:center')
```