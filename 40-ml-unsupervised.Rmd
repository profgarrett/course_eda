# Unsupervised Learning 

``` {r setup, echo = FALSE, include = FALSE}
library(tidyverse)

# Open up a dataset for use
#   150 cases, 5 variables: Sepal.Length, Sepal.Width, Petal.Length, Petal.Width, Species
data("iris")

# Lookup help on it by calling ?iris

# Turn into a tibble
t_iris <- tibble(iris)
```


  
## k-means

k-means clustering is an technique that automatically assigns individual data points to groups. It's useful for finding natural groups in your data.

Here are some resources:

- Conceptual overview: [k-means: an explorable explainer](https://k-means-explorable.vercel.app/)
- R code walkthrough: [Unsupervised Learning: k-means](https://lgatto.github.io/IntroMachineLearningWithR/unsupervised-learning.html#k-means-clustering)
- Deep dive [Hands-on machine learning with R](https://bradleyboehmke.github.io/HOML/kmeans.html)


We will be using the iris dataset extensively. 

!(iris picture)[https://upload.wikimedia.org/wikipedia/commons/thumb/9/9f/Iris_virginica.jpg/1280px-Iris_virginica.jpg]


### Concept

We are going to try and use the sepal/petal lengths/widths to automatically identify the 3 different types of iris flowers.

Below shows one sample plot, which compares the Sepal Length against the Petal Length. But, we have a lot of possible combinations.

``` {r}
# Plot petals
ggplot( data=t_iris, mapping = aes( x=Sepal.Length, y=Petal.Length, color=Species)) +
  geom_point() + 
  labs( title = 'Iris dataset: species variable')
```

We can use k-means to see which combination of attributes lets us have the highest accuracy.

``` {r}

# Run the kmeans algorithm:
#   x is a tibble, which should only have numeric data
#   centers is the number of clusters we want
#   nstart says how many random starting points we should try out (default to 10)
kresult <- kmeans(
  x = select(t_iris, Sepal.Length, Petal.Length),
  centers = 3,
  nstart = 10
)

print(kresult)
```

Look at the results of k-means to see how well the algorithm worked.

There are several useful outputs:

- `cluster`: vector of numbers, one per input row, showing the cluster for each row. Use this to assign each of your original data points to a cluster.
- `centers`: matrix of cluster centers. Use this to plot the center of each cluster on a chart.
- `tot.withiness`: total within cluster sum of squares, i.e. `sum(withinss)`. We want this to be as small, so each cluster is tightly defined. 
- `between`: between-cluster sum of squares. We want this to be large, showing that each cluster is far away from the other clusters.
- `size`: number of points in each cluster
- `iter`: number of times the algorithm ran before stopping

To see a visualization of sum of square, see https://shiny.rit.albany.edu/stat/visualizess/


### Minimal example

This is a minimal example, but can be significantly improved in the full template.

``` {r}

# Run the kmeans algorithm:
kresult <- kmeans(
  x = select(t_iris, Sepal.Length, Petal.Length),
  centers = 3,
)

# Add the new kmeans2 column, using kresult's cluster vector.
t_iris <- mutate(t_iris, kmeans = (kresult$cluster))

# Plot petals
ggplot(data = t_iris) + 
  geom_point(mapping = aes( x = Sepal.Length, y = Petal.Length, color = kmeans) 
  )

```

### Full example

``` {r}

# Give a seed value so that we get the same results each time
set.seed(1)

# Run the kmeans algorithm:
#   x is a tibble, which should only have numeric data
#   centers is the number of clusters we want
#   nstart says how many random starting points we should try out (default to 10)
kresult <- kmeans(
  x = select(t_iris, Sepal.Length, Petal.Length),
  centers = 3,
  nstart = 10
)

# Add the new kmeans2 column, using kresult's cluster vector.
# Use factor to turn the cluster from a number to a categorical variable.
t_iris <- mutate(t_iris, kmeans = factor(kresult$cluster))

# Plot petals
ggplot() + 
  # Plot each point
  geom_point(
    data = t_iris, 
    mapping = aes( x = Sepal.Length, y = Petal.Length, color = kmeans) 
  ) +
  # Plot the center of each cluster 
  geom_point( 
    data = as_tibble(kresult$centers), 
    mapping = aes( x = Sepal.Length, y = Petal.Length), 
    size = 6, 
    shape = 3) +
  labs( 
    title = 'Iris dataset: k-means clustering',
    subtitle = paste('Error:', round(kresult$tot.withinss,2))
    )

```



## Decision Trees

Decision-trees automatically create questions to split our data into groups. They are very useful for creating human-understandable models of a problem space.

Help resources:

- [Visualizations of DT](https://explained.ai/decision-tree-viz/index.html)
- [Supervised Learning](https://lgatto.github.io/IntroMachineLearningWithR/supervised-learning.html)
- [HOML](https://bradleyboehmke.github.io/HOML/DT.html)

### Concept

We follow this process:

- Select an output variable
- Go through each input variable to find the best split in our output
- Split the dataset
- For each split, repeat until we reach a good ending condition

### Minimal Example

Below creates a visualization of a decision tree. 

Note the output for each node gives the 0.3 (proportion) of items in each class, as well as the % of rows that find their way to this node.

``` {r}
library(tidyverse)
library(rpart)
library(rpart.plot)

# Load data into a tibble
data(iris)
t.iris <- iris

# Create the model
#   formula: output_variable ~ input_field_a + input_field_b + ...
#   data: your tibble, excluding the output variable.
#   method: what type of problem are we working with?
#     class - predicting a discrete variable
#     anova - regression for a value
m <- rpart(formula = Species ~ Sepal.Length + 
                      Sepal.Width + Petal.Length + Petal.Width, 
           data = t.iris,
           method = "class")

# Plot results of model
rpart.plot(m)
```

What if we want the output of the plot as a number showing its accuracy? Add the following lines

``` {r}

# Give the tibble to the model, and generate a vector of predicted
# output class
predicted <- predict(m, t.iris, type = 'class')

# Add the predicted results to each row in the tibble as a field.
predicted_as_str <- paste('Predicted', predicted)
t.iris <- mutate( t.iris, predicted = predicted_as_str )

# Show a confusion matrix
table(t.iris$predicted, t.iris$Species)

```


### Full example

Several other features are very helpful in pruning our decision tree.

``` {r}
library(tidyverse, rpart, rpart.plot)

# Load data into a tibble
data(iris)
t.iris <- iris


## Split the data into training / testing

# Find the number of rows
count_of_rows <- length(t.iris$Species)
# Create a random vector of 1s and 0s the same length
select01 <- rbinom(count_of_rows, 1, .5)


## Create a training / test set

# Add the 01 vector to the original tibble
t.iris <- mutate(t.iris, test01 = select01) 
# Create a test and train tibble with this as a filter
t.iris.test <- filter(t.iris, test01 == 1)
t.iris.train <- filter(t.iris, test01 == 0)


## Create the model with TRAIN tibble
# 
#   minsplit: a number with the min number of rows required for a split
#   minbucket: a number wiht the min number of rows required for a bucket
m <- rpart(formula = Species ~ Sepal.Length + 
                      Sepal.Width + Petal.Length + Petal.Width, 
           data = t.iris.train,
           method = "class",
           minsplit = 2,
           minbucket = 2)

# Plot results of model with the TRAINING data
rpart.plot(m)


## Predict on the TEST tibble

# Create a vector of prediction results
predicted <- predict(m, t.iris.test, type = 'class')
# Add to our train tibble.
t.iris.test <- mutate(t.iris.test, predicted = predicted)

# Show results
table(str_to_upper(predicted), t.iris.test$Species)
```